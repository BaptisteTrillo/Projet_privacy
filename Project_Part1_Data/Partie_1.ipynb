{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import pour le code en g√©n√©ral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from datetime import timedelta\n",
    "import movingpandas as mpd\n",
    "import hvplot.pandas\n",
    "import json\n",
    "import contextily as ctx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import pykalman\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the data from the csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les fichiers CSV\n",
    "activity_df = pd.read_csv(\"Activity_self_report.csv\", sep=';', parse_dates=['time'])\n",
    "print(\"1. Donn√©es des activit√©s charg√©es :\\n\", activity_df.head())\n",
    "\n",
    "gps_log_df = pd.read_csv(\"GPS_log.csv\", sep=';', parse_dates=['timestamp'])\n",
    "print(\"2. Donn√©es GPS charg√©es :\\n\", gps_log_df.head())\n",
    "\n",
    "sensor_measures_df = pd.read_csv(\"Sensor_Measures.csv\", sep=';', parse_dates=['time'])\n",
    "print(\"3. Donn√©es des capteurs charg√©es :\\n\", sensor_measures_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Segment the data per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer les fuseaux horaires pour √©viter les erreurs\n",
    "gps_log_df['timestamp'] = gps_log_df['timestamp'].dt.tz_localize(None)\n",
    "activity_df['time'] = activity_df['time'].dt.tz_localize(None)\n",
    "sensor_measures_df['time'] = sensor_measures_df['time'].dt.tz_localize(None)\n",
    "print(\"4. Fuseaux horaires supprim√©s des donn√©es\")\n",
    "print(\"2. Donn√©es GPS charg√©es :\\n\", gps_log_df.head())\n",
    "print(\"2. Donn√©es activite charg√©es :\\n\", activity_df.head())\n",
    "print(\"2. Donn√©es sensor charg√©es :\\n\", sensor_measures_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîπ Ajouter une colonne 'date' en extrayant uniquement YYYY-MM-DD\n",
    "gps_log_df[\"date\"] = gps_log_df[\"timestamp\"].dt.date\n",
    "activity_df[\"date\"] = activity_df[\"time\"].dt.date\n",
    "sensor_measures_df[\"date\"] = sensor_measures_df[\"time\"].dt.date\n",
    "\n",
    "# üîπ V√©rifier les premi√®res valeurs\n",
    "print(\"Donn√©es GPS segment√©es par jour :\\n\", gps_log_df.head())\n",
    "print(\"Donn√©es d'activit√©s segment√©es par jour :\\n\", activity_df.head())\n",
    "print(\"Donn√©es capteurs segment√©es par jour :\\n\", sensor_measures_df.head())\n",
    "\n",
    "# üîπ Grouper les donn√©es par jour (optionnel)\n",
    "gps_grouped = gps_log_df.groupby(\"date\")\n",
    "activity_grouped = activity_df.groupby(\"date\")\n",
    "sensor_grouped = sensor_measures_df.groupby(\"date\")\n",
    "\n",
    "# üîπ Afficher un aper√ßu des groupes\n",
    "print(f\"Nombre de jours distincts dans les donn√©es GPS : {len(gps_grouped)}\")\n",
    "print(f\"Nombre de jours distincts dans les activit√©s : {len(activity_grouped)}\")\n",
    "print(f\"Nombre de jours distincts dans les capteurs : {len(sensor_grouped)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Transform the scalar data to spatial type in geopandas and spatiotemporal type in movingpandas and visualize them. You may create different versions to represent the data at each stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GeoDataFrame \n",
    "gps_log_df['geometry'] = [Point(xy) for xy in zip(gps_log_df['lon'], gps_log_df['lat'])]\n",
    "gdf = gpd.GeoDataFrame(gps_log_df, geometry='geometry')\n",
    "gdf.set_crs(epsg=4326, inplace=True)  # Assuming the coordinates are in WGS84\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cr√©er des points g√©om√©triques pour les donn√©es GPS\n",
    "geometry = [Point(xy) for xy in zip(gps_log_df['lon'], gps_log_df['lat'])]\n",
    "gdf = gpd.GeoDataFrame(gps_log_df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "#Ajouter un identifiant de trajectoire bas√© sur l'utilisateur ou une logique arbitraire\n",
    "if 'user_id' in gdf.columns:\n",
    "    gdf['trajectory_id'] = gdf['user_id']\n",
    "else:\n",
    "    gdf['trajectory_id'] = gdf.index // 100  # Exemple : diviser en blocs arbitraires\n",
    "\n",
    "#Transformation en TrajectoryCollection avec MovingPandas\n",
    "trajectory_collection = mpd.TrajectoryCollection(gdf, traj_id_col='trajectory_id', t='timestamp')\n",
    "\n",
    "#Validation des trajectoires cr√©√©es\n",
    "print(f\"Nombre de trajectoires d√©tect√©es : {len(trajectory_collection)}\")\n",
    "for traj in trajectory_collection:\n",
    "    print(traj)\n",
    "\n",
    "#Visualisation des trajectoires pour validation\n",
    "map_visualization = gdf.hvplot(geo=True, tiles='OSM', c='trajectory_id', line_width=2, width=800, height=600)\n",
    "map_visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'trajectory_collection' is already created and contains trajectories\n",
    "for trajectory in trajectory_collection:\n",
    "    print(trajectory)\n",
    "    # Adding distance in default units (meters) and custom units (kilometers and yards)\n",
    "    trajectory.add_distance(overwrite=True, name=\"distance_m\", units=\"m\")\n",
    "    trajectory.add_distance(overwrite=True, name=\"distance_km\", units=\"km\")\n",
    "    trajectory.add_distance(overwrite=True, name=\"distance_yards\", units=\"yd\")\n",
    "\n",
    "    # Adding speed in default units (m/s) and custom units (feet/minute and knots)\n",
    "    trajectory.add_speed(overwrite=True, name=\"speed_kmph\", units=(\"km\", \"h\"))\n",
    "    trajectory.add_speed(overwrite=True, name=\"speed_ft_min\", units=(\"ft\", \"min\"))\n",
    "    trajectory.add_speed(overwrite=True, name=\"speed_knots\", units=(\"nm\", \"h\"))\n",
    "\n",
    "    # Correctly adding acceleration with appropriate time units\n",
    "    trajectory.add_acceleration(overwrite=True, name=\"acceleration_mps2\", units=(\"m\", \"s\", \"s\"))\n",
    "    trajectory.add_acceleration(overwrite=True, name=\"acceleration_mph_s\", units=(\"mi\", \"h\", \"s\"))\n",
    "\n",
    "\n",
    "    # Print the updated dataframe to see the added columns\n",
    "    print(trajectory.df.head())\n",
    "\n",
    "#Visualisation des trajectoires pour validation\n",
    "map_visualization = gdf.hvplot(geo=True, tiles='OSM', c='trajectory_id', line_width=2, width=800, height=600)\n",
    "map_visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For interactive visualization with hvplot\n",
    "import hvplot.pandas\n",
    "hvplot_defaults = {\n",
    "    \"tiles\": None,\n",
    "    \"frame_height\": 320,\n",
    "    \"frame_width\": 320,\n",
    "    \"cmap\": 'Viridis',\n",
    "    \"colorbar\": True\n",
    "}\n",
    "trajectory.hvplot(c='speed_kmph', **hvplot_defaults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Clean the trajectories and the temporal measures and check the result visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage des trajectoires avec suppression des outliers\n",
    "cleaned_trajectories = []\n",
    "nb_trajectoires_initiales = len(trajectory_collection.trajectories)\n",
    "\n",
    "for trajectory in trajectory_collection.trajectories:  \n",
    "    cleaner = mpd.OutlierCleaner(trajectory)  \n",
    "    cleaned_traj = cleaner.clean(alpha=2)  # Alpha d√©finit la tol√©rance aux outliers\n",
    "    \n",
    "    # V√©rifier si la trajectoire nettoy√©e contient encore des donn√©es\n",
    "    if cleaned_traj is not None and not cleaned_traj.df.empty:\n",
    "        cleaned_traj.add_speed(overwrite=True, units=(\"km\", \"h\"))  # Recalculer la vitesse\n",
    "        cleaned_trajectories.append(cleaned_traj)\n",
    "        print(f\"‚úÖ Trajectoire {trajectory.id} nettoy√©e : {len(trajectory.df)} ‚Üí {len(cleaned_traj.df)} points restants\")\n",
    "    else:\n",
    "        print(f\"‚ùå Trajectoire {trajectory.id} supprim√©e apr√®s nettoyage (trop d'outliers)\")\n",
    "\n",
    "# V√©rification du nombre de trajectoires conserv√©es\n",
    "nb_trajectoires_nettoyees = len(cleaned_trajectories)\n",
    "print(f\"\\nüìå Nombre de trajectoires avant nettoyage : {nb_trajectoires_initiales}\")\n",
    "print(f\"üìå Nombre de trajectoires apr√®s nettoyage : {nb_trajectoires_nettoyees}\")\n",
    "\n",
    "# Cr√©ation d'une nouvelle collection avec les trajectoires nettoy√©es\n",
    "cleaned_trajectory_collection = mpd.TrajectoryCollection(cleaned_trajectories)\n",
    "\n",
    "# Visualisation interactive des trajectoires nettoy√©es\n",
    "gdf_cleaned = gpd.GeoDataFrame(pd.concat([t.df for t in cleaned_trajectories]), crs=\"EPSG:4326\")\n",
    "map_cleaned = gdf_cleaned.hvplot(\n",
    "    geo=True, tiles='OSM', c='trajectory_id', line_width=2, width=800, height=600\n",
    ")\n",
    "map_cleaned\n",
    "\n",
    "# Option : Visualisation statique avec une carte de fond\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "gdf_cleaned.plot(ax=ax, alpha=0.7, edgecolor='k')\n",
    "ctx.add_basemap(ax, crs=gdf_cleaned.crs, source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "plt.title(\"Trajectoires apr√®s nettoyage des outliers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean each trajectory by removing outliers\n",
    "cleaned_trajectories = []\n",
    "for trajectory in trajectory_collection:\n",
    "    # Split into segments if needed and apply OutlierCleaner\n",
    "    cleaner = mpd.OutlierCleaner(trajectory)\n",
    "    cleaned_traj = cleaner.clean(alpha=2)  # Alpha defines how strict the threshold is\n",
    "    cleaned_traj.add_speed(overwrite=True, units=(\"km\", \"h\"))  # Re-add speed for visualization\n",
    "    cleaned_trajectories.append(cleaned_traj)\n",
    "    print(f\"Cleaned Trajectory for User {trajectory.id}\")\n",
    "    print(cleaned_traj.df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize after cleaning\n",
    "import hvplot.pandas  # Required for interactive visualization\n",
    "\n",
    "hvplot_defaults = {\n",
    "    \"tiles\": \"CartoLight\",\n",
    "    \"frame_height\": 400,\n",
    "    \"frame_width\": 500,\n",
    "    \"cmap\": \"Viridis\",\n",
    "    \"colorbar\": True,\n",
    "}\n",
    "\n",
    "# Example: Plot the cleaned trajectory for the first user\n",
    "trajectory_collection.trajectories[0].hvplot(\n",
    "    label=\"Before Cleaning\", color=\"red\", line_width=4, **hvplot_defaults\n",
    ") * cleaned_trajectories[0].hvplot(\n",
    "    label=\"After Cleaning\", c=\"speed\", line_width=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Detect the stops (visits) and the moves (travels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er une collection de trajectoires\n",
    "tc = mpd.TrajectoryCollection(cleaned_trajectories, 'user_id', t='time')\n",
    "\n",
    "# Initialiser le d√©tecteur de stops\n",
    "detector = mpd.TrajectoryStopDetector(tc)\n",
    "\n",
    "# D√©tecter les arr√™ts (Stops)\n",
    "stop_segments = detector.get_stop_segments(min_duration=timedelta(minutes=7), max_diameter=100)\n",
    "\n",
    "# Cr√©er une liste pour stocker les moves\n",
    "move_segments = []\n",
    "\n",
    "# Identifier les moves comme √©tant les p√©riodes entre les stops\n",
    "for traj in tc.trajectories:\n",
    "    stop_times = [(stop.df.index.min(), stop.df.index.max()) for stop in stop_segments]\n",
    "\n",
    "    # Trier les stops pour √©viter les erreurs\n",
    "    stop_times.sort()\n",
    "\n",
    "    prev_end = traj.df.index.min()  # D√©but de la trajectoire\n",
    "\n",
    "    for start, end in stop_times:\n",
    "        # Extraire les moves entre la fin du stop pr√©c√©dent et le d√©but du stop actuel\n",
    "        move_df = traj.df[(traj.df.index > prev_end) & (traj.df.index < start)]\n",
    "        \n",
    "        # V√©rifier qu'il y a au moins 2 points avant d'ajouter\n",
    "        if len(move_df) >= 2:\n",
    "            move_segments.append(mpd.Trajectory(move_df, traj.id))\n",
    "\n",
    "        prev_end = end  # Mettre √† jour pour le prochain move\n",
    "\n",
    "# V√©rification des r√©sultats\n",
    "print(f\"‚úÖ Nombre de stops d√©tect√©s : {len(stop_segments)}\")\n",
    "print(f\"‚úÖ Nombre de moves d√©tect√©s : {len(move_segments)}\")\n",
    "\n",
    "# Convertir les r√©sultats en GeoDataFrames\n",
    "stop_gdf = stop_segments.to_point_gdf()\n",
    "move_gdf = gpd.GeoDataFrame(pd.concat([traj.to_line_gdf() for traj in move_segments]), crs=\"EPSG:4326\")\n",
    "\n",
    "# Afficher les premiers r√©sultats\n",
    "print(\"‚úÖ Stops d√©tect√©s :\")\n",
    "print(stop_gdf.head())\n",
    "\n",
    "print(\"\\n‚úÖ Moves d√©tect√©s :\")\n",
    "print(move_gdf.head())\n",
    "\n",
    "# Sauvegarder les r√©sultats dans des fichiers CSV\n",
    "stop_gdf.to_csv(\"Detected_Stops.csv\", index=False)\n",
    "move_gdf.to_csv(\"Detected_Moves.csv\", index=False)\n",
    "\n",
    "print(\"\\nüìÇ R√©sultats export√©s : 'Detected_Stops.csv' et 'Detected_Moves.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Segment the data based on the stops and moves ; propagate the segmentation to the measurement part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les mesures capteurs\n",
    "sensor_measures_df = pd.read_csv(\"Sensor_Measures.csv\", sep=';', parse_dates=['time'])\n",
    "\n",
    "# Trier les mesures pour √©viter les erreurs de correspondance\n",
    "sensor_measures_df = sensor_measures_df.sort_values(by=\"time\")\n",
    "\n",
    "# **Correction : Supprimer les fuseaux horaires**\n",
    "sensor_measures_df['time'] = sensor_measures_df['time'].dt.tz_localize(None)\n",
    "\n",
    "# Appliquer la correction sur les stops et moves\n",
    "for stop in stop_segments:\n",
    "    stop.df.index = stop.df.index.tz_localize(None)\n",
    "\n",
    "for move in move_segments:\n",
    "    move.df.index = move.df.index.tz_localize(None)\n",
    "\n",
    "# Ajouter une colonne 'Segment_Type' aux mesures capteurs\n",
    "sensor_measures_df['Segment_Type'] = np.nan\n",
    "sensor_measures_df['Segment_ID'] = np.nan  # Ajout de l'ID du segment pour analyse\n",
    "\n",
    "# Associer chaque mesure capteur au segment le plus proche\n",
    "for stop in stop_segments:\n",
    "    mask = (sensor_measures_df['time'] >= stop.df.index.min()) & (sensor_measures_df['time'] <= stop.df.index.max())\n",
    "    sensor_measures_df.loc[mask, 'Segment_Type'] = 'Stop'\n",
    "    sensor_measures_df.loc[mask, 'Segment_ID'] = stop.id  # Associer l'ID du segment Stop\n",
    "\n",
    "for move in move_segments:\n",
    "    mask = (sensor_measures_df['time'] >= move.df.index.min()) & (sensor_measures_df['time'] <= move.df.index.max())\n",
    "    sensor_measures_df.loc[mask, 'Segment_Type'] = 'Move'\n",
    "    sensor_measures_df.loc[mask, 'Segment_ID'] = move.id  # Associer l'ID du segment Move\n",
    "\n",
    "# Supprimer les mesures sans segment associ√©\n",
    "sensor_measures_df.dropna(subset=['Segment_Type'], inplace=True)\n",
    "\n",
    "# Afficher les premiers r√©sultats\n",
    "print(sensor_measures_df.head())\n",
    "\n",
    "# Sauvegarder le fichier segment√©\n",
    "sensor_measures_df.to_csv(\"Segmented_Sensor_Data.csv\", index=False)\n",
    "print(\"üìÇ Donn√©es segment√©es export√©es : 'Segmented_Sensor_Data.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot stops\n",
    "hvplot_defaults = {\n",
    "    \"tiles\": \"CartoLight\",\n",
    "    \"frame_height\": 400,\n",
    "    \"frame_width\": 600,\n",
    "    \"cmap\": \"Viridis\",\n",
    "    \"colorbar\": True\n",
    "}\n",
    "tc_plot = tc.hvplot(color=\"slategray\", **hvplot_defaults)\n",
    "stop_segment_plot = stop_segments.hvplot(line_width=7.0, color=\"deeppink\")\n",
    "(tc_plot * stop_segment_plot).opts(title=\"Stops and Moves Visualization\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Does the result conform to the self-reported changes? Discuss the gaps; try different parameters in the preprocessing, such as the stop duration, smoothing trajectories beforehand, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finition des cat√©gories d'activit√©s\n",
    "stops = {\"Bureau\", \"Domicile\", \"Magasin\", \"Parc\"}\n",
    "moves = {\"Rue\", \"Bus\", \"Train\", \"M√©tro\"}\n",
    "\n",
    "# Ajouter une colonne 'Type' pour classer les activit√©s\n",
    "activity_df['Type'] = activity_df['activity'].apply(lambda x: \"Stop\" if x in stops else \"Move\" if x in moves else \"Unknown\")\n",
    "\n",
    "# Calculer la dur√©e de chaque activit√© (diff√©rence entre les timestamps)\n",
    "activity_df['duration'] = activity_df['time'].diff().shift(-1)\n",
    "\n",
    "# Afficher un aper√ßu\n",
    "print(activity_df)\n",
    "\n",
    "# Visualisation du temps pass√© en \"Stop\" vs \"Move\"\n",
    "summary = activity_df.groupby(\"Type\")[\"duration\"].sum()\n",
    "summary_hours = summary.dt.total_seconds() / 3600  # Convertir en heures\n",
    "\n",
    "# Affichage sous forme de diagramme\n",
    "plt.figure(figsize=(6, 4))\n",
    "summary_hours.plot(kind='bar', color=['green', 'red'])\n",
    "plt.ylabel(\"Temps total (heures)\")\n",
    "plt.title(\"Temps pass√© en 'Stop' et 'Move'\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finir un ID de segment unique pour chaque p√©riode de Stop ou Move\n",
    "activity_df['Segment_ID'] = (activity_df['Type'] != activity_df['Type'].shift()).cumsum()\n",
    "\n",
    "# Fusionner avec le DataFrame des mesures pour propager la segmentation\n",
    "sensor_measures_df = sensor_measures_df.sort_values(by=\"time\")  # Assurer l'ordre chronologique\n",
    "\n",
    "# Associer chaque mesure au bon segment (on prend le segment en cours au moment de la mesure)\n",
    "sensor_measures_df['Segment_ID'] = np.nan  # Initialisation\n",
    "\n",
    "for i, row in activity_df.iterrows():\n",
    "    # Trouver les mesures qui se situent entre cette activit√© et la suivante\n",
    "    if i < len(activity_df) - 1:\n",
    "        mask = (sensor_measures_df['time'] >= row['time']) & (sensor_measures_df['time'] < activity_df.loc[i+1, 'time'])\n",
    "    else:\n",
    "        mask = sensor_measures_df['time'] >= row['time']\n",
    "    \n",
    "    # Associer le Segment_ID et le Type\n",
    "    sensor_measures_df.loc[mask, 'Segment_ID'] = row['Segment_ID']\n",
    "    sensor_measures_df.loc[mask, 'Type'] = row['Type']\n",
    "\n",
    "# Nettoyage final\n",
    "sensor_measures_df.dropna(subset=['Segment_ID'], inplace=True)  # Supprimer les mesures sans segment associ√©\n",
    "\n",
    "# Afficher un aper√ßu des mesures segment√©es\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(sensor_measures_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trier les donn√©es pour un bon alignement temporel\n",
    "sensor_measures_df = sensor_measures_df.sort_values(by=\"time\")\n",
    "activity_df = activity_df.sort_values(by=\"time\")\n",
    "\n",
    "# Comparer en utilisant une fusion approximative (tol√©rance de 30 secondes)\n",
    "comparison_df = pd.merge_asof(\n",
    "    sensor_measures_df[['time', 'Segment_Type']],  # Automatique\n",
    "    activity_df[['time', 'Type']],  # Manuelle\n",
    "    on='time',\n",
    "    direction='nearest',  # Prend la valeur la plus proche\n",
    "    tolerance=pd.Timedelta(seconds=30)  # Tol√©rance de 30 sec\n",
    ")\n",
    "\n",
    "# Renommer les colonnes pour plus de clart√©\n",
    "comparison_df.rename(columns={'Segment_Type': 'Type_detected', 'Type': 'Type_reported'}, inplace=True)\n",
    "\n",
    "# Supprimer les lignes o√π il n'y a pas d'auto-d√©claration (√©vite les NaN)\n",
    "comparison_df.dropna(subset=['Type_reported'], inplace=True)\n",
    "\n",
    "# D√©tecter les incoh√©rences\n",
    "comparison_df['Mismatch'] = comparison_df['Type_detected'] != comparison_df['Type_reported']\n",
    "\n",
    "# Compter les erreurs\n",
    "nb_mismatches = comparison_df['Mismatch'].sum()\n",
    "total_entries = len(comparison_df)\n",
    "\n",
    "print(f\"Nombre total de diff√©rences entre les donn√©es d√©tect√©es et auto-d√©clar√©es : {nb_mismatches} / {total_entries}\")\n",
    "print(f\"Erreur de d√©tection : {round((nb_mismatches / total_entries) * 100, 2)}%\")\n",
    "\n",
    "# Afficher les cas probl√©matiques\n",
    "print(\"\\nExemples d'incoh√©rences d√©tect√©es :\")\n",
    "print(comparison_df[comparison_df['Mismatch']].head(10))\n",
    "\n",
    "# Table de confusion entre les cat√©gories\n",
    "labels = [\"Stop\", \"Move\"]\n",
    "cm = confusion_matrix(comparison_df[\"Type_reported\"], comparison_df[\"Type_detected\"], labels=labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "\n",
    "# Affichage de la table de confusion\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "disp.plot(ax=ax, cmap=\"Blues\")\n",
    "plt.title(\"Table de confusion : D√©tections vs Auto-d√©clarations\")\n",
    "plt.show()\n",
    "\n",
    "# Visualisation des erreurs avec un diagramme √† barres\n",
    "plt.figure(figsize=(6, 4))\n",
    "comparison_df['Mismatch'].value_counts().plot(kind='bar', color=['green', 'red'])\n",
    "plt.xticks([0, 1], ['Correspondance', 'Incoh√©rence'], rotation=0)\n",
    "plt.ylabel(\"Nombre de cas\")\n",
    "plt.title(\"Correspondance entre auto-d√©clarations et d√©tections\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les donn√©es GPS\n",
    "gps_log_df = pd.read_csv(\"GPS_log.csv\", sep=';', parse_dates=['timestamp'])\n",
    "\n",
    "# Suppression des valeurs nulles √©ventuelles\n",
    "gps_log_df = gps_log_df.dropna(subset=['lat', 'lon'])\n",
    "\n",
    "# Appliquer un filtre de Kalman pour lisser les coordonn√©es GPS\n",
    "kalman_filter = pykalman.KalmanFilter(\n",
    "    initial_state_mean=[gps_log_df['lat'].iloc[0], gps_log_df['lon'].iloc[0]],\n",
    "    transition_matrices=np.eye(2),\n",
    "    observation_matrices=np.eye(2),\n",
    "    initial_state_covariance=1e-4 * np.eye(2),\n",
    "    observation_covariance=1e-1 * np.eye(2),\n",
    "    transition_covariance=1e-4 * np.eye(2)\n",
    ")\n",
    "\n",
    "# Ex√©cuter le filtre sur les coordonn√©es\n",
    "filtered_state_means, _ = kalman_filter.filter(gps_log_df[['lat', 'lon']].values)\n",
    "\n",
    "# Ajouter les nouvelles coordonn√©es liss√©es dans le DataFrame\n",
    "gps_log_df['lat_smooth'] = filtered_state_means[:, 0]\n",
    "gps_log_df['lon_smooth'] = filtered_state_means[:, 1]\n",
    "\n",
    "# Cr√©er des GeoDataFrames pour visualisation\n",
    "gps_log_df['geometry'] = [Point(xy) for xy in zip(gps_log_df['lon'], gps_log_df['lat'])]\n",
    "gps_log_df['geometry_smooth'] = [Point(xy) for xy in zip(gps_log_df['lon_smooth'], gps_log_df['lat_smooth'])]\n",
    "\n",
    "gdf = gpd.GeoDataFrame(gps_log_df, geometry='geometry', crs=\"EPSG:4326\")\n",
    "gdf_smooth = gpd.GeoDataFrame(gps_log_df, geometry='geometry_smooth', crs=\"EPSG:4326\")\n",
    "\n",
    "# Visualisation avant/apr√®s filtrage\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "gdf.plot(ax=ax, alpha=0.5, label=\"Trajectoire brute\", color=\"red\")\n",
    "gdf_smooth.plot(ax=ax, alpha=0.8, label=\"Trajectoire liss√©e\", color=\"blue\")\n",
    "plt.legend()\n",
    "plt.title(\"Comparaison des trajectoires brutes et liss√©es (Filtre de Kalman)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Use movingpandas to export the trajectories (each stop and move segments) in MF-JSON format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir les stops en Trajectoires\n",
    "stop_trajectories = [mpd.Trajectory(stop.df, stop.id) for stop in stop_segments if len(stop.df) > 1]\n",
    "\n",
    "# Convertir les moves en Trajectoires\n",
    "move_trajectories = [mpd.Trajectory(move.df, move.id) for move in move_segments if len(move.df) > 1]\n",
    "\n",
    "# Cr√©er une collection contenant les Stops\n",
    "stop_collection = mpd.TrajectoryCollection(stop_trajectories)\n",
    "\n",
    "# Cr√©er une collection contenant les Moves\n",
    "move_collection = mpd.TrajectoryCollection(move_trajectories)\n",
    "\n",
    "# Convertir les collections en GeoDataFrame\n",
    "gdf_stops = gpd.GeoDataFrame(pd.concat([traj.to_line_gdf() for traj in stop_collection.trajectories]), crs=\"EPSG:4326\")\n",
    "gdf_moves = gpd.GeoDataFrame(pd.concat([traj.to_line_gdf() for traj in move_collection.trajectories]), crs=\"EPSG:4326\")\n",
    "\n",
    "# Exporter en GeoJSON (compatible MF-JSON)\n",
    "stop_filename = \"stops_segmented.geojson\"\n",
    "move_filename = \"moves_segmented.geojson\"\n",
    "\n",
    "gdf_stops.to_file(stop_filename, driver=\"GeoJSON\")\n",
    "gdf_moves.to_file(move_filename, driver=\"GeoJSON\")\n",
    "\n",
    "print(f\"‚úÖ Stops export√©s en MF-JSON : {stop_filename}\")\n",
    "print(f\"‚úÖ Moves export√©s en MF-JSON : {move_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Load the pre-processed data into MobilityDB using the MF-JSON object obtained previously as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de la connexion PostgreSQL (MobilityDB dans Docker)\n",
    "DB_NAME = \"mobilitydb\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASSWORD = \"postgres\"\n",
    "DB_HOST = \"localhost\"  # Ou l'IP de ton conteneur\n",
    "DB_PORT = \"5433\"  # Assure-toi que c'est bien le port expos√©\n",
    "\n",
    "# Connexion √† PostgreSQL\n",
    "engine = create_engine(f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\")\n",
    "\n",
    "print(\"‚úÖ Connexion √† MobilityDB r√©ussie !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les fichiers GeoJSON\n",
    "stops_gdf = gpd.read_file(\"stops_segmented.geojson\")\n",
    "moves_gdf = gpd.read_file(\"moves_segmented.geojson\")\n",
    "\n",
    "# Ajouter une colonne pour identifier Stop ou Move\n",
    "stops_gdf[\"segment_type\"] = \"Stop\"\n",
    "moves_gdf[\"segment_type\"] = \"Move\"\n",
    "\n",
    "# Fusionner les deux DataFrames\n",
    "full_gdf = pd.concat([stops_gdf, moves_gdf])\n",
    "\n",
    "print(f\"üìä Nombre de segments charg√©s : {len(full_gdf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nom de la table dans MobilityDB\n",
    "TABLE_NAME = \"mobility_trajectories\"\n",
    "\n",
    "# Importer les donn√©es dans PostgreSQL\n",
    "full_gdf.to_postgis(TABLE_NAME, engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "print(f\"‚úÖ Donn√©es import√©es dans la table {TABLE_NAME} de MobilityDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier la structure de la table\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT column_name FROM information_schema.columns WHERE table_name = 'mobility_trajectories';\"))\n",
    "    columns = [row[0] for row in result.fetchall()]\n",
    "    print(\"üìã Colonnes existantes :\", columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. For each stop segment, assign a centroid location, its time range and its duration, as well as the maximum distance in meters to the bounding box (this reflects the uncertainty of the location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier les colonnes disponibles dans la table mobility_trajectories\n",
    "query = text(\"\"\"\n",
    "    SELECT column_name \n",
    "    FROM information_schema.columns \n",
    "    WHERE table_name = 'mobility_trajectories';\n",
    "\"\"\")\n",
    "\n",
    "# Ex√©cuter la requ√™te et r√©cup√©rer les colonnes\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(query)\n",
    "    columns = [row[0] for row in result.fetchall()]\n",
    "\n",
    "# Afficher les colonnes disponibles pour adapter la requ√™te\n",
    "print(\"üìã Colonnes disponibles dans 'mobility_trajectories':\", columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requ√™te SQL corrig√©e pour MobilityDB\n",
    "query = text(\"\"\"\n",
    "    WITH StopAggregates AS (\n",
    "        SELECT \n",
    "            traj_id,\n",
    "            ST_Centroid(ST_Collect(geometry)) AS centroid, \n",
    "            MIN(t) AS start_time, \n",
    "            MAX(t) AS end_time, \n",
    "            (MAX(t) - MIN(t)) AS duration\n",
    "        FROM mobility_trajectories \n",
    "        WHERE segment_type = 'Stop'\n",
    "        GROUP BY traj_id\n",
    "    )\n",
    "    SELECT \n",
    "        s.traj_id,\n",
    "        s.centroid,\n",
    "        s.start_time,\n",
    "        s.end_time,\n",
    "        s.duration,\n",
    "        MAX(ST_Distance(m.geometry, s.centroid)) AS max_distance_bbox \n",
    "    FROM mobility_trajectories m\n",
    "    JOIN StopAggregates s ON m.traj_id = s.traj_id\n",
    "    WHERE m.segment_type = 'Stop'\n",
    "    GROUP BY s.traj_id, s.centroid, s.start_time, s.end_time, s.duration;\n",
    "\"\"\")\n",
    "\n",
    "# Ex√©cuter la requ√™te et charger les r√©sultats dans un DataFrame\n",
    "with engine.connect() as conn:\n",
    "    stop_summary = pd.read_sql(query, conn)\n",
    "\n",
    "# Sauvegarder les r√©sultats dans un fichier CSV\n",
    "output_path = \"Stop_Summary.csv\"\n",
    "stop_summary.to_csv(output_path, index=False)\n",
    "\n",
    "# Afficher un message de confirmation\n",
    "print(f\"‚úÖ R√©sum√© des Stops enregistr√© dans '{output_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Find recurrent stops:matchthe stop segments based on the distance between the centroids and the intersection of their box, and assign them the same tag or stop_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requ√™te SQL pour d√©tecter les Stops r√©currents\n",
    "query = text(\"\"\"\n",
    "    WITH StopAggregates AS (\n",
    "        SELECT \n",
    "            traj_id,\n",
    "            ST_Centroid(ST_Collect(geometry)) AS centroid,\n",
    "            ST_Envelope(ST_Collect(geometry)) AS bbox,\n",
    "            MIN(t) AS start_time,\n",
    "            MAX(t) AS end_time,\n",
    "            (MAX(t) - MIN(t)) AS duration\n",
    "        FROM mobility_trajectories \n",
    "        WHERE segment_type = 'Stop'\n",
    "        GROUP BY traj_id\n",
    "    )\n",
    "    SELECT \n",
    "        sa1.traj_id AS stop_id_1,\n",
    "        sa2.traj_id AS stop_id_2,\n",
    "        ST_Distance(sa1.centroid, sa2.centroid) AS centroid_distance,\n",
    "        ST_Intersects(sa1.bbox, sa2.bbox) AS bbox_intersection\n",
    "    FROM StopAggregates sa1\n",
    "    JOIN StopAggregates sa2\n",
    "    ON sa1.traj_id < sa2.traj_id\n",
    "    WHERE ST_Distance(sa1.centroid, sa2.centroid) < 30\n",
    "    AND ST_Intersects(sa1.bbox, sa2.bbox) = TRUE;\n",
    "\"\"\")\n",
    "\n",
    "# Ex√©cuter la requ√™te et charger les r√©sultats dans un DataFrame\n",
    "with engine.connect() as conn:\n",
    "    recurrent_stops = pd.read_sql(query, conn)\n",
    "\n",
    "# Sauvegarder les r√©sultats dans un fichier CSV\n",
    "output_path = \"Recurrent_Stops.csv\"\n",
    "recurrent_stops.to_csv(output_path, index=False)\n",
    "\n",
    "# Afficher un message de confirmation\n",
    "print(f\"‚úÖ D√©tection des Stops r√©currents termin√©e ! R√©sultats enregistr√©s dans '{output_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Rank the stops by the frequency of visits(the most visited first).Rank them by the total time spent at the stop. Tag the 1st ranked stop as ‚ÄúHome‚Äù and the second as ‚ÄúWork‚Äù."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîπ V√©rifier si la table \"Recurrent_Stops\" existe\n",
    "check_table_query = text(\"\"\"\n",
    "    SELECT table_name \n",
    "    FROM information_schema.tables \n",
    "    WHERE table_name = 'recurrent_stops';\n",
    "\"\"\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(check_table_query)\n",
    "    table_exists = result.fetchone() is not None\n",
    "\n",
    "if not table_exists:\n",
    "    print(\"üö® La table 'Recurrent_Stops' n'existe pas. Cr√©ation en cours...\")\n",
    "\n",
    "    # üîπ Cr√©ation de la table \"Recurrent_Stops\" si elle n'existe pas\n",
    "    create_recurrent_stops_query = text(\"\"\"\n",
    "        CREATE TABLE Recurrent_Stops AS \n",
    "        WITH StopAggregates AS (\n",
    "            SELECT \n",
    "                traj_id,\n",
    "                ST_Centroid(ST_Collect(geometry)) AS centroid,\n",
    "                ST_Envelope(ST_Collect(geometry)) AS bbox,\n",
    "                MIN(t) AS start_time,\n",
    "                MAX(t) AS end_time,\n",
    "                (MAX(t) - MIN(t)) AS duration\n",
    "            FROM mobility_trajectories \n",
    "            WHERE segment_type = 'Stop'\n",
    "            GROUP BY traj_id\n",
    "        )\n",
    "        SELECT \n",
    "            sa1.traj_id AS stop_id_1,\n",
    "            sa2.traj_id AS stop_id_2,\n",
    "            ST_Distance(sa1.centroid, sa2.centroid) AS centroid_distance,\n",
    "            ST_Intersects(sa1.bbox, sa2.bbox) AS bbox_intersection\n",
    "        FROM StopAggregates sa1\n",
    "        JOIN StopAggregates sa2\n",
    "        ON sa1.traj_id < sa2.traj_id\n",
    "        WHERE ST_Distance(sa1.centroid, sa2.centroid) < 30\n",
    "        AND ST_Intersects(sa1.bbox, sa2.bbox) = TRUE;\n",
    "    \"\"\")\n",
    "\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(create_recurrent_stops_query)\n",
    "        conn.commit()  # üîπ Valider la cr√©ation\n",
    "        print(\"‚úÖ Table 'Recurrent_Stops' cr√©√©e avec succ√®s !\")\n",
    "\n",
    "else:\n",
    "    print(\"‚úÖ La table 'Recurrent_Stops' existe d√©j√†.\")\n",
    "\n",
    "# üîπ V√©rifier si la table contient des donn√©es\n",
    "check_data_query = text(\"SELECT COUNT(*) FROM Recurrent_Stops;\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(check_data_query)\n",
    "    row_count = result.scalar()\n",
    "\n",
    "if row_count == 0:\n",
    "    print(\"‚ö†Ô∏è La table 'Recurrent_Stops' est vide. V√©rifie les donn√©es dans 'mobility_trajectories'.\")\n",
    "else:\n",
    "    print(f\"üìä La table 'Recurrent_Stops' contient {row_count} enregistrements.\")\n",
    "\n",
    "# üîπ Ex√©cuter la requ√™te de classement des Stops\n",
    "rank_stops_query = text(\"\"\"\n",
    "    WITH StopDurations AS (\n",
    "        SELECT \n",
    "            trajectory_id AS stop_id,\n",
    "            (MAX(t) - MIN(t)) AS duration  -- Calculer la dur√©e de chaque Stop\n",
    "        FROM mobility_trajectories\n",
    "        WHERE segment_type = 'Stop'\n",
    "        GROUP BY trajectory_id\n",
    "    ),\n",
    "    StopRanking AS (\n",
    "        SELECT \n",
    "            rs.stop_id_1 AS stop_id, \n",
    "            SUM(sd.duration) AS total_time_spent  -- Calculer la dur√©e totale pass√©e aux stops\n",
    "        FROM Recurrent_Stops rs\n",
    "        JOIN StopDurations sd \n",
    "        ON rs.stop_id_1 = sd.stop_id\n",
    "        GROUP BY rs.stop_id_1\n",
    "    )\n",
    "    SELECT \n",
    "        stop_id,\n",
    "        total_time_spent,\n",
    "        CASE \n",
    "            WHEN RANK() OVER (ORDER BY total_time_spent DESC) = 1 THEN 'Home'\n",
    "            WHEN RANK() OVER (ORDER BY total_time_spent DESC) = 2 THEN 'Work'\n",
    "            ELSE ''\n",
    "        END AS Label\n",
    "    FROM StopRanking\n",
    "    ORDER BY total_time_spent DESC;\n",
    "\"\"\")\n",
    "\n",
    "# üîπ Ex√©cuter la requ√™te et charger les r√©sultats dans un DataFrame\n",
    "with engine.connect() as conn:\n",
    "    ranked_stops = pd.read_sql(rank_stops_query, conn)\n",
    "\n",
    "# üîπ Sauvegarder les r√©sultats dans un fichier CSV\n",
    "output_path = \"Ranked_Stops.csv\"\n",
    "ranked_stops.to_csv(output_path, index=False)\n",
    "\n",
    "# ‚úÖ Afficher un message de confirmation\n",
    "print(f\"‚úÖ Classement des Stops termin√© ! R√©sultats enregistr√©s dans '{output_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Rank the ‚Äúmove segments‚Äù by the total distance traversed.Rank them by duration. Rank the stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_moves_query = text(\"\"\"\n",
    "    WITH MoveDurations AS (\n",
    "        SELECT \n",
    "            trajectory_id AS move_id,\n",
    "            SUM(distance_m) AS total_distance,  -- Distance totale en m√®tres\n",
    "            (MAX(t) - MIN(t)) AS total_duration -- Dur√©e totale du Move\n",
    "        FROM mobility_trajectories\n",
    "        WHERE segment_type = 'Move'\n",
    "        GROUP BY trajectory_id\n",
    "    )\n",
    "    SELECT \n",
    "        move_id,\n",
    "        total_distance,\n",
    "        total_duration,\n",
    "        RANK() OVER (ORDER BY total_distance DESC) AS rank_by_distance,\n",
    "        RANK() OVER (ORDER BY total_duration DESC) AS rank_by_duration\n",
    "    FROM MoveDurations\n",
    "    ORDER BY total_distance DESC, total_duration DESC;\n",
    "\"\"\")\n",
    "\n",
    "# üîπ Ex√©cuter la requ√™te et charger les r√©sultats dans un DataFrame\n",
    "with engine.connect() as conn:\n",
    "    ranked_moves = pd.read_sql(rank_moves_query, conn)\n",
    "\n",
    "# üîπ Sauvegarder les r√©sultats dans un fichier CSV\n",
    "output_path = \"Ranked_Moves.csv\"\n",
    "ranked_moves.to_csv(output_path, index=False)\n",
    "\n",
    "# ‚úÖ Afficher un message de confirmation\n",
    "print(f\"‚úÖ Classement des Move Segments termin√© ! R√©sultats enregistr√©s dans '{output_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_stops_query = text(\"\"\"\n",
    "    WITH StopDurations AS (\n",
    "        SELECT \n",
    "            trajectory_id AS stop_id,\n",
    "            MAX(t) - MIN(t) AS duration,  -- Calculer la dur√©e de chaque visite\n",
    "            COUNT(*) AS visit_count  -- Nombre de visites au Stop\n",
    "        FROM mobility_trajectories\n",
    "        WHERE segment_type = 'Stop'\n",
    "        GROUP BY trajectory_id\n",
    "    ),\n",
    "    StopRanking AS (\n",
    "        SELECT \n",
    "            stop_id,\n",
    "            visit_count,\n",
    "            SUM(duration) AS total_time_spent  -- Somme des dur√©es de toutes les visites\n",
    "        FROM StopDurations\n",
    "        GROUP BY stop_id, visit_count\n",
    "    )\n",
    "    SELECT \n",
    "        stop_id,\n",
    "        visit_count,\n",
    "        total_time_spent,\n",
    "        RANK() OVER (ORDER BY visit_count DESC) AS rank_by_frequency,\n",
    "        RANK() OVER (ORDER BY total_time_spent DESC) AS rank_by_duration\n",
    "    FROM StopRanking\n",
    "    ORDER BY total_time_spent DESC, visit_count DESC;\n",
    "\"\"\")\n",
    "# üîπ Ex√©cuter la requ√™te et afficher les r√©sultats directement\n",
    "with engine.connect() as conn:\n",
    "    ranked_stops = pd.read_sql(rank_stops_query, conn)\n",
    "\n",
    "# üîπ Afficher le DataFrame sous forme de tableau\n",
    "print(ranked_stops)\n",
    "\n",
    "# üîπ Visualisation : Histogramme des stops par dur√©e totale\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(ranked_stops[\"stop_id\"].astype(str), ranked_stops[\"total_time_spent\"], color='blue')\n",
    "plt.xlabel(\"Stop ID\")\n",
    "plt.ylabel(\"Total Time Spent (seconds)\")\n",
    "plt.title(\"Ranking of Stops by Total Time Spent\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# üîπ Visualisation : Histogramme des stops par fr√©quence de visite\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(ranked_stops[\"stop_id\"].astype(str), ranked_stops[\"visit_count\"], color='green')\n",
    "plt.xlabel(\"Stop ID\")\n",
    "plt.ylabel(\"Visit Count\")\n",
    "plt.title(\"Ranking of Stops by Frequency of Visits\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Find recurrent trajectory paths : use different methods to identify trajectories that share the same path (e.g., similar traversed distance, distance between start and end location, or between the whole linear shapes, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_distances = text(\"\"\"\n",
    "    SELECT trajectory_id, SUM(distance_m) AS total_distance \n",
    "    FROM mobility_trajectories\n",
    "    WHERE segment_type = 'Move'\n",
    "    GROUP BY trajectory_id\n",
    "    ORDER BY total_distance DESC\n",
    "    LIMIT 10;\n",
    "\"\"\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    df_distances = pd.read_sql(query_distances, conn)\n",
    "\n",
    "print(\"üìè Distances des 10 plus longs trajets 'Move' :\")\n",
    "print(df_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_recurrent_traj = text(\"\"\"\n",
    "    WITH TrajectoryFeatures AS (\n",
    "        SELECT \n",
    "            trajectory_id,\n",
    "            SUM(distance_m) AS total_distance,  \n",
    "            ST_StartPoint(ST_Collect(geometry)) AS start_point,\n",
    "            ST_EndPoint(ST_Collect(geometry)) AS end_point\n",
    "        FROM mobility_trajectories\n",
    "        WHERE segment_type = 'Move'\n",
    "        GROUP BY trajectory_id\n",
    "    )\n",
    "    SELECT \n",
    "        t1.trajectory_id AS traj_1, \n",
    "        t2.trajectory_id AS traj_2,\n",
    "        ABS(t1.total_distance - t2.total_distance) AS distance_diff,  \n",
    "        ST_Distance(t1.start_point, t2.start_point) AS start_distance,  \n",
    "        ST_Distance(t1.end_point, t2.end_point) AS end_distance  \n",
    "    FROM TrajectoryFeatures t1\n",
    "    JOIN TrajectoryFeatures t2\n",
    "    ON t1.trajectory_id < t2.trajectory_id\n",
    "    LIMIT 10;\n",
    "\"\"\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    df_recurrent = pd.read_sql(query_recurrent_traj, conn)\n",
    "\n",
    "print(\"üîç Exemples de trajets r√©currents trouv√©s :\")\n",
    "print(df_recurrent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_check_moves = text(\"\"\"\n",
    "    SELECT trajectory_id, COUNT(*) AS nb_points, SUM(distance_m) AS total_distance\n",
    "    FROM mobility_trajectories\n",
    "    WHERE segment_type = 'Move'\n",
    "    GROUP BY trajectory_id\n",
    "    ORDER BY nb_points DESC;\n",
    "\"\"\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    move_stats = pd.read_sql(query_check_moves, conn)\n",
    "\n",
    "print(\"üìä Distribution des distances des trajets Move :\")\n",
    "print(move_stats.head(10))\n",
    "\n",
    "# üîπ Visualisation de la distribution des distances\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(move_stats[\"total_distance\"].dropna(), bins=30, color='blue', edgecolor='black')\n",
    "plt.xlabel(\"Total Distance (meters)\")\n",
    "plt.ylabel(\"Number of Trajectories\")\n",
    "plt.title(\"Distribution of Move Trajectories by Distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recurrent_trajectories_query = text(\"\"\"\n",
    "    WITH TrajectoryFeatures AS (\n",
    "        SELECT \n",
    "            trajectory_id,\n",
    "            SUM(distance_m) AS total_distance,  \n",
    "            ST_StartPoint(ST_Collect(geometry)) AS start_point,  \n",
    "            ST_EndPoint(ST_Collect(geometry)) AS end_point,  \n",
    "            ST_Collect(geometry) AS trajectory_shape  \n",
    "        FROM mobility_trajectories\n",
    "        WHERE segment_type = 'Move'\n",
    "        GROUP BY trajectory_id\n",
    "    )\n",
    "    SELECT \n",
    "        t1.trajectory_id AS traj_1, \n",
    "        t2.trajectory_id AS traj_2,\n",
    "        ABS(t1.total_distance - t2.total_distance) AS distance_diff,  \n",
    "        ST_Distance(t1.start_point, t2.start_point) AS start_distance,  \n",
    "        ST_Distance(t1.end_point, t2.end_point) AS end_distance,  \n",
    "        ST_HausdorffDistance(t1.trajectory_shape, t2.trajectory_shape) AS shape_distance  \n",
    "    FROM TrajectoryFeatures t1\n",
    "    JOIN TrajectoryFeatures t2\n",
    "    ON t1.trajectory_id < t2.trajectory_id\n",
    "    WHERE \n",
    "        ABS(t1.total_distance - t2.total_distance) < 100  \n",
    "        AND ST_Distance(t1.start_point, t2.start_point) < 50  \n",
    "        AND ST_Distance(t1.end_point, t2.end_point) < 50  \n",
    "        AND ST_HausdorffDistance(t1.trajectory_shape, t2.trajectory_shape) < 200;\n",
    "\"\"\")\n",
    "\n",
    "# üîπ Ex√©cuter la requ√™te et charger les r√©sultats dans un DataFrame\n",
    "with engine.connect() as conn:\n",
    "    recurrent_trajectories = pd.read_sql(recurrent_trajectories_query, conn)\n",
    "\n",
    "# üîπ Afficher les r√©sultats\n",
    "print(\"üîç Exemples de trajets r√©currents trouv√©s (version optimis√©e) :\")\n",
    "print(recurrent_trajectories.head())\n",
    "\n",
    "# üîπ Visualisation des diff√©rences de distance\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(recurrent_trajectories[\"distance_diff\"].dropna(), bins=20, color='green', edgecolor='black')\n",
    "plt.xlabel(\"Difference in Distance (meters)\")\n",
    "plt.ylabel(\"Number of Similar Trajectories\")\n",
    "plt.title(\"Distribution of Recurrent Trajectories by Distance\")\n",
    "plt.show()\n",
    "\n",
    "# üîπ Visualisation des distances de Hausdorff\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(recurrent_trajectories[\"shape_distance\"].dropna(), bins=20, color='red', edgecolor='black')\n",
    "plt.xlabel(\"Hausdorff Distance (meters)\")\n",
    "plt.ylabel(\"Number of Similar Trajectories\")\n",
    "plt.title(\"Distribution of Recurrent Trajectories by Shape Similarity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîπ V√©rification des distances moyennes et des points par trajectoire\n",
    "query_avg_distance = text(\"\"\"\n",
    "    SELECT COUNT(*) AS nb_trajets, AVG(total_distance) AS avg_distance, MIN(total_distance) AS min_distance, MAX(total_distance) AS max_distance\n",
    "    FROM (\n",
    "        SELECT trajectory_id, SUM(distance_m) AS total_distance\n",
    "        FROM mobility_trajectories\n",
    "        WHERE segment_type = 'Move'\n",
    "        GROUP BY trajectory_id\n",
    "    ) AS traj_dist;\n",
    "\"\"\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    distance_stats = pd.read_sql(query_avg_distance, conn)\n",
    "\n",
    "print(\"üìä Statistiques des distances de trajets 'Move' :\")\n",
    "print(distance_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15 Let‚Äôs consider that the air quality score (AQS) is a function of PM2.5, PM10 and NO2. After normalizing these variables (e.g., by Min-Max feature scaling), the score is obtained by the mean value of the three normalized variables. Compute this temporal score under mobilityDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import text\n",
    "import pandas as pd\n",
    "\n",
    "# Requ√™te SQL pour calculer AQS en g√©rant les valeurs NULL\n",
    "aqs_query = text(\"\"\"\n",
    "    WITH MinMaxValues AS (\n",
    "        SELECT \n",
    "            MIN(pm25) AS min_pm25, MAX(pm25) AS max_pm25, AVG(pm25) AS avg_pm25,\n",
    "            MIN(pm10) AS min_pm10, MAX(pm10) AS max_pm10, AVG(pm10) AS avg_pm10,\n",
    "            MIN(no2) AS min_no2, MAX(no2) AS max_no2, AVG(no2) AS avg_no2\n",
    "        FROM sensor_measures\n",
    "    ),\n",
    "    NormalizedData AS (\n",
    "        SELECT \n",
    "            sm.time,\n",
    "            (COALESCE(sm.pm25, mm.avg_pm25) - mm.min_pm25) / NULLIF(mm.max_pm25 - mm.min_pm25, 0) AS pm25_norm,\n",
    "            (COALESCE(sm.pm10, mm.avg_pm10) - mm.min_pm10) / NULLIF(mm.max_pm10 - mm.min_pm10, 0) AS pm10_norm,\n",
    "            (COALESCE(sm.no2, mm.avg_no2) - mm.min_no2) / NULLIF(mm.max_no2 - mm.min_no2, 0) AS no2_norm\n",
    "        FROM sensor_measures sm, MinMaxValues mm\n",
    "    )\n",
    "    SELECT \n",
    "        time,\n",
    "        (pm25_norm + pm10_norm + no2_norm) / 3 AS air_quality_score\n",
    "    FROM NormalizedData\n",
    "    ORDER BY time;\n",
    "\"\"\")\n",
    "\n",
    "# üîπ Ex√©cuter la requ√™te et charger les r√©sultats dans un DataFrame\n",
    "with engine.connect() as conn:\n",
    "    aqs_df = pd.read_sql(aqs_query, conn)\n",
    "\n",
    "# üîπ Afficher les premiers r√©sultats\n",
    "print(\"\\n‚úÖ Calcul de l'Air Quality Score (AQS) termin√© !\\n\")\n",
    "print(aqs_df.head(10))  # Affiche les 10 premi√®res lignes\n",
    "\n",
    "# Optionnel : Sauvegarder les r√©sultats dans un fichier CSV\n",
    "aqs_df.to_csv(\"Air_Quality_Score.csv\", index=False)\n",
    "print(\"\\nüìÇ R√©sultats enregistr√©s dans 'Air_Quality_Score.csv'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. Rank the days per ascending cumulated AQS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_days_query = text(\"\"\"\n",
    "    WITH DailyAQS AS (\n",
    "        SELECT \n",
    "            DATE(time) AS day,\n",
    "            SUM(air_quality_score) AS total_aqs\n",
    "        FROM Air_Quality_Score\n",
    "        GROUP BY DATE(time)\n",
    "    )\n",
    "    SELECT \n",
    "        day,\n",
    "        total_aqs,\n",
    "        RANK() OVER (ORDER BY total_aqs ASC) AS rank\n",
    "    FROM DailyAQS\n",
    "    ORDER BY total_aqs ASC;\n",
    "\"\"\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    ranked_days_df = pd.read_sql(rank_days_query, conn)\n",
    "\n",
    "print(\"‚úÖ Classement des jours par AQS cumul√© termin√© !\")\n",
    "print(ranked_days_df.head())  # Afficher un aper√ßu\n",
    "\n",
    "# üìÇ Sauvegarde en CSV\n",
    "output_path = \"Ranked_Days_AQS.csv\"\n",
    "ranked_days_df.to_csv(output_path, index=False)\n",
    "print(f\"üìÇ R√©sultats enregistr√©s dans '{output_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17. Compare the cumulated AQS , the min ,max, and average between days and nights ,and between the stops and the moves periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_aqs_query = text(\"\"\"\n",
    "    WITH AggregatedAQS AS (\n",
    "        SELECT \n",
    "            period,\n",
    "            segment,\n",
    "            SUM(air_quality_score) AS total_aqs,\n",
    "            MIN(air_quality_score) AS min_aqs,\n",
    "            MAX(air_quality_score) AS max_aqs,\n",
    "            AVG(air_quality_score) AS avg_aqs\n",
    "        FROM Air_Quality_Score\n",
    "        GROUP BY period, segment\n",
    "    )\n",
    "    SELECT * FROM AggregatedAQS;\n",
    "\"\"\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    comparison_df = pd.read_sql(compare_aqs_query, conn)\n",
    "\n",
    "print(\"‚úÖ Comparaison des scores AQS entre les p√©riodes termin√©e !\")\n",
    "print(comparison_df)\n",
    "\n",
    "# üìÇ Sauvegarde en CSV pour analyse\n",
    "output_path = \"AQS_Comparison.csv\"\n",
    "comparison_df.to_csv(output_path, index=False)\n",
    "print(f\"üìÇ R√©sultats enregistr√©s dans '{output_path}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
